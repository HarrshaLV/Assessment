# -*- coding: utf-8 -*-
"""LVADSUSR89_FA_Harrsha_lab4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e16sQVn1oK7MaGjr-CJ2hi2o3nbQCSqk
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

df = pd.read_csv('/content/social_network.csv')
df.columns

df.head()

plt.figure(figsize=(16,9))
sns.heatmap(data=df.corr(),annot=True)

from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.ensemble import IsolationForest

print(df.isnull().sum())

sns.countplot(x = df['suspicious_activity'],palette='rainbow')

"""#Handling Outliers

"""

model_outlier = IsolationForest(contamination=0.1, random_state=42)
outliers = model_outlier.fit_predict(df[['login_activity', 'posting_activity', 'social_connections']])
df['is_an_outlier'] = outliers

"""#Encoding Data"""

label_encoder = LabelEncoder()
df['account_status'] = label_encoder.fit_transform(df['account_status'])

features = ['login_activity', 'posting_activity', 'social_connections']
X = df[features]
y = df['is_an_outlier']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = IsolationForest(n_estimators=100, contamination=0.1, max_features=3, max_samples=10000, random_state=42)
model.fit(X_train)

# Predict the anomalies in the data
y_pred = model.predict(X_train)
df["anomaly_score"] = model.decision_function(X)

anomalies = df.loc[df["anomaly_score"] < 0]

plt.scatter(df["social_connections"], df["anomaly_score"], label="Not an Anomaly")
plt.scatter(anomalies["social_connections"], anomalies["anomaly_score"], color="r", label="Anomaly")
plt.xlabel("Social Connections")
plt.ylabel("Anomaly Score")
plt.title("Scatter plot for Social Connections and Anomaly Score")
plt.legend(loc='lower right')
plt.show()